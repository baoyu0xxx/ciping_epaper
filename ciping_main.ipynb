{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 基于jieba库分词和roberta进行词频统计"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 所有文本词频统计"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import jieba\n",
    "import concurrent.futures\n",
    "from collections import Counter\n",
    "import tqdm\n",
    "\n",
    "def load_stopwords(filepath, seed_dic_path):\n",
    "    \"\"\"加载停用词和种子词词典\"\"\"\n",
    "    jieba.load_userdict(seed_dic_path)\n",
    "    with open(filepath, 'r', encoding='utf-8') as f:\n",
    "        return set(f.read().splitlines())\n",
    "\n",
    "def process_text_file(filepath, stopwords, word_freq):\n",
    "    \"\"\"处理单个文本文件并更新词频统计\"\"\"\n",
    "    with open(filepath, 'r', encoding='utf-8') as f:\n",
    "        content = f.read()\n",
    "        words = jieba.cut_for_search(content)\n",
    "        filtered_words = (word for word in words if word not in stopwords and len(word.strip()) > 1 and not word.isdigit())\n",
    "        word_freq.update(filtered_words)\n",
    "\n",
    "def process_text_files_concurrently(directory, stopwords):\n",
    "    \"\"\"并发处理目录下的文本文件，统计词频\"\"\"\n",
    "    word_freq = Counter()\n",
    "    # 获取所有txt文件路径\n",
    "    txt_files = [os.path.join(root, f) for root, _, files in os.walk(directory) for f in files if f.endswith('.txt')]\n",
    "    \n",
    "    with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "        with tqdm.tqdm(total=len(txt_files), desc=\"Processing files\") as pbar:\n",
    "            futures = []\n",
    "            for filepath in txt_files:\n",
    "                future = executor.submit(process_text_file, filepath, stopwords, word_freq)\n",
    "                future.add_done_callback(lambda p: pbar.update())\n",
    "                futures.append(future)\n",
    "            \n",
    "            concurrent.futures.wait(futures)\n",
    "    \n",
    "    return word_freq\n",
    "\n",
    "\n",
    "def del_word(word_freq_df,freq = 5,sort_store = False):\n",
    "    \"\"\"\n",
    "    删除频次小于 freq 的词语，删除包含%或.的词语\n",
    "\n",
    "    @param word_freq_df: 词频统计结果\n",
    "    @param freq: 频次阈值\n",
    "    @param sort_store: 是否保存筛选前词频统计结果\n",
    "    \"\"\"\n",
    "    if not sort_store:\n",
    "        word_freq_df = word_freq_df[word_freq_df['频次'] >= freq]\n",
    "        word_freq_df = word_freq_df[~word_freq_df['词语'].str.contains('%|\\.')]\n",
    "        word_freq_df.to_csv('cipingdata/word_freq.csv', index=False, encoding='utf-8')\n",
    "    else:\n",
    "        word_freq_df.to_csv('cipingdata/word_freq_beforefilt.csv', index=False, encoding='utf-8')\n",
    "        word_freq_df = word_freq_df[word_freq_df['频次'] >= freq]\n",
    "        word_freq_df = word_freq_df[~word_freq_df['词语'].str.contains('%|\\.')]\n",
    "        word_freq_df.to_csv('cipingdata/word_freq.csv', index=False, encoding='utf-8')\n",
    "\n",
    "    print(f'词频统计结果已保存至 cipingdata/word_freq.csv') \n",
    "\n",
    "# 主程序\n",
    "if __name__ == \"__main__\":\n",
    "    firm_annanls_path = 'cipingdata/testtxt' # 年报文本路径\n",
    "    stopwords_dir = 'cipingdata/stopwords.txt' # 停用词文件路径\n",
    "    seed_dic_path = 'cipingdata/seed_dict.csv' # 种子词词典路径\n",
    "\n",
    "    stopwords = load_stopwords(stopwords_dir, seed_dic_path)\n",
    "    word_freq = process_text_files_concurrently(firm_annanls_path, stopwords)\n",
    "\n",
    "    # 转换成 DataFrame 并按频次排序\n",
    "    word_freq_df = pd.DataFrame(word_freq.items(), columns=['词语', '频次'])\n",
    "    word_freq_df = word_freq_df.sort_values(by='频次', ascending=False)\n",
    "    del_word(word_freq_df,freq = 5,sort_store = False) # 删除频次小于5的词语，删除包含%或.的词语"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## roberta余弦相似度构建词典"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 输出近似词列表——需自行筛选优化近似词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "import tqdm\n",
    "\n",
    "# 计算给定词的词向量表示\n",
    "def get_word_vector(word: str, tokenizer, model, device) -> torch.Tensor:\n",
    "    inputs = tokenizer(word, return_tensors='pt').to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        embeddings = outputs.last_hidden_state.mean(dim=1).squeeze()\n",
    "    return embeddings\n",
    "\n",
    "# 计算余弦相似度\n",
    "def cosine_similarity(vec1: torch.Tensor, vec2: torch.Tensor) -> float:\n",
    "    return F.cosine_similarity(vec1.unsqueeze(0), vec2.unsqueeze(0)).item()\n",
    "\n",
    "# 初始化模型和设备\n",
    "def initialize_model(model_path: str):\n",
    "    tokenizer = BertTokenizer.from_pretrained(model_path)\n",
    "    model = BertModel.from_pretrained(model_path)\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model.to(device)\n",
    "    return tokenizer, model, device\n",
    "\n",
    "# 加载数据\n",
    "def load_data(seed_dict_path: str, word_freq_filtered_path: str):\n",
    "    seed_dict = pd.read_csv(seed_dict_path, encoding='utf-8', header=None)\n",
    "    word_freq_filtered = pd.read_csv(word_freq_filtered_path, encoding='utf-8')\n",
    "    word_freq_filtered_list = word_freq_filtered['词语'].tolist()\n",
    "    return seed_dict, word_freq_filtered_list\n",
    "\n",
    "# 获取词向量\n",
    "def get_word_vectors(word_list, tokenizer, model, device, batch_size=32):\n",
    "    word_vectors = {}\n",
    "    for i in tqdm.tqdm(range(0, len(word_list), batch_size), desc=\"Calculating word vectors in batches\"):\n",
    "        word_batch = word_list[i:i + batch_size]\n",
    "        inputs = tokenizer(word_batch, return_tensors='pt', padding=True, truncation=True).to(device)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "            embeddings = outputs.last_hidden_state.mean(dim=1)\n",
    "        for word, embedding in zip(word_batch, embeddings):\n",
    "            word_vectors[word] = embedding\n",
    "        torch.cuda.empty_cache()\n",
    "    return word_vectors\n",
    "\n",
    "# 处理AI字典\n",
    "def process_ai_dict(seed_dict, word_vectors, tokenizer, model, device):\n",
    "    similar_words_dict = []\n",
    "    for word in tqdm.tqdm(seed_dict[0], desc=\"Processing AI dictionary\"):\n",
    "        word_vector_ai = get_word_vector(word, tokenizer, model, device)\n",
    "        similarities = [(ref_word, cosine_similarity(word_vector_ai, ref_vector)) for ref_word, ref_vector in word_vectors.items()]\n",
    "        top_10_similar = sorted(similarities, key=lambda x: x[1], reverse=True)[:10]\n",
    "        similar_words_dict.append({\n",
    "            'Word': word,\n",
    "            'Top_10_Similar_Words': [word for word, _ in top_10_similar],\n",
    "            'Similarity_Scores': [score for _, score in top_10_similar]\n",
    "        })\n",
    "    return similar_words_dict\n",
    "\n",
    "\n",
    "# 主函数\n",
    "def main(model_path,seed_dict_path,word_freq_filtered_path,similar_words_path,output_path):\n",
    "    \n",
    "    tokenizer, model, device = initialize_model(model_path)\n",
    "    seed_dict, word_freq_filtered_list = load_data(seed_dict_path, word_freq_filtered_path)\n",
    "    word_vectors = get_word_vectors(word_freq_filtered_list, tokenizer, model, device)\n",
    "    similar_words_dict = process_ai_dict(seed_dict, word_vectors, tokenizer, model, device)\n",
    "    \n",
    "    similar_words_df = pd.DataFrame(similar_words_dict)\n",
    "    similar_words_df.to_csv(similar_words_path, index=False, encoding='utf-8')\n",
    "    \n",
    "    similar_words_df = pd.read_csv(similar_words_path, encoding='utf-8')\n",
    "    words = pd.DataFrame(similar_words_df['Word'], columns=['词语'])\n",
    "\n",
    "    similar_words_list = [word for row in similar_words_df['Top_10_Similar_Words'].tolist() for word in eval(row)]\n",
    "    similar_words_list = list(set(similar_words_list))\n",
    "    # 输出所有近似词\n",
    "    similar_words_list = pd.DataFrame(similar_words_list, columns=['词语'])\n",
    "\n",
    "    words_df = pd.merge(words,similar_words_list,how='outer',on='词语')\n",
    "    words_df.to_csv(output_path, index=False, encoding='utf-8', header=False)\n",
    "    \n",
    "    print(f'\\n所有近似词已输出至 {output_path} \\n共计 {len(similar_words_list)} 个词语')\n",
    "    print(f'请用户自行筛选，删除不相关词语')\n",
    "    print(f'也可以利用下面提供的代码进行筛选')\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    model_path = 'hfl/chinese-roberta-wwm-ext'\n",
    "    seed_dict_path = 'cipingdata/seed_dict.csv'\n",
    "    word_freq_path = 'cipingdata/word_freq.csv'\n",
    "    similar_words_path = 'cipingdata/similar_words.csv'\n",
    "    output_path = 'cipingdata/words_all.csv'\n",
    "\n",
    "    main(model_path,seed_dict_path,word_freq_path,similar_words_path,output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 无关词剔除"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# 剔除无关词\n",
    "def filter_rubbish_words(words_list, rubbish_words=[]):\n",
    "    # 若未传入垃圾词列表，则使用默认垃圾词列表\n",
    "    if not rubbish_words:\n",
    "        rubbish_words = [\n",
    "        '电子产品','现代农业','知识','网上超市','验算','节能',\n",
    "        '电智','计算方法','整合营销','录音机','语音','客服部',\n",
    "        '用户服务','提取液','表意','网络营销','特约服务','市场营销',\n",
    "        '金融监管','智慧之门','运动用品','风控','风控全','知识表示',\n",
    "        '感应式','汽车音响','传感器','专业音响','风控部','计算长度',\n",
    "        '变矩器','重复性','推进改革','衰减系数','服务中心','英语教学',\n",
    "        '中国银联','热线服务','特约服务','网上商城','注重实效','指代',\n",
    "        '工智','智慧网','运输系统','勤奋学习','RPK','有问必答','网商小贷',\n",
    "        '表意','特殊性','现代医学','GCK','意指','知识','周期性地',\n",
    "        '生活必需品','RBR','读取数据','泛用','含义','问答','提取液',\n",
    "        '孝敬老人','电子商城','脱虚向实','保健用品','间断性','老有所养',\n",
    "        '电智','真实世界','知识型','新营销','验算','促进改革','强化班',\n",
    "        '提问','用户服务','努力学习','微笑服务','客服',\n",
    "        '时尚家居','节约能源','节能','具有特征','养老','电子产品',\n",
    "        '交易系统','术语','虚心学习','计算','理论知识','物联新','金属产品',\n",
    "        'UDEM','题库系统','家居生活','应答机','音响器','科普知识','现实主义',\n",
    "        '灯管影响','防护系统','及风控','养老险','尊老敬老','掌上电脑',\n",
    "        '课后复习','养老院','金属制品','证券监管','短期投资','医学知识',\n",
    "        '外资保险','认真学习','通识性','CCAR','客户服务','HVLP',\n",
    "        '电动汽车','LATALimited','记忆','代名词','TCXO','QDLP',\n",
    "        '绿色生态','定量分析','m3a','灯光音响','市场监管','智化',\n",
    "        'LMEshield','节能降耗','物联云商','环保','集中学习','分束器',\n",
    "        '固有特征','红外摄像机','形状记忆','可视化','VB1','多媒体通信',\n",
    "        '学习材料','TLP','应答器','识别','问题解答','安全监管','成本计算',\n",
    "        'YBR','层次性','RPCB','节省能源','色谱分析','有源音箱','数据表示',\n",
    "        '相控阵','生态农业','惯性导航','生物芯片','学科知识','咨询服务',\n",
    "        '向量','温度传感器','据智研','计算速度','人机界面','激光雷达',\n",
    "        '人工控制','数码产品','网上银行','科学知识','科学实践','音频芯片',\n",
    "        '节能产品','电子政务','基本知识','交通运输业','检索系统','数学计算',\n",
    "        '数据库','自动自发','手机芯片','计算机芯片','学习效果','计算机控制',\n",
    "        '工控微机','中枢神经','密集式','科技知识','物联科','神经学',\n",
    "        '养老保险','识别性','高智','NVME','NVMe','高科技产品',\n",
    "        '统计数据','变量','分布式应用','NVMeSSD','计算力','芯片业',\n",
    "        ]\n",
    "        print(f'采用默认垃圾词列表，共计 {len(rubbish_words)} 个词语')\n",
    "    \n",
    "    words = pd.read_csv('cipingdata/words_all.csv', encoding='utf-8', header=None, names=['词语'])\n",
    "    words_list = [word for word in words_list if word not in rubbish_words]\n",
    "    words_df = pd.DataFrame(words_list, columns=['词语'])\n",
    "    words_df = pd.merge(words_df, words, how='inner', on='词语')\n",
    "\n",
    "    words_df.to_csv('cipingdata/words_all.csv', index=False, encoding='utf-8',header=False)\n",
    "\n",
    "\n",
    "print(f'若用户已经在 words_all.csv 中手动删除了不相关词语，请跳过此步骤！')\n",
    "# print(f'当然，也可以使用下面的代码删除垃圾词')\n",
    "# print(f'但是注意：默认垃圾词列表为 AI 词典中的词语，请视情况使用')\n",
    "\n",
    "rubbish_words = [] # 垃圾词列表\n",
    "words_path = 'cipingdata/words_all.csv' # 词语列表路径\n",
    "words_list = pd.read_csv(words_path, encoding='utf-8', header=None)[0].tolist()\n",
    "              \n",
    "filter_rubbish_words(words_list = words_list, rubbish_words=rubbish_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 形成整体词频统计"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import jieba\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "def process_file(file_info):\n",
    "    \"\"\"处理单个文件，统计AI关键词词频\"\"\"\n",
    "    company_name, year, report_path = file_info\n",
    "    ai_word_count = 0\n",
    "    \n",
    "    if os.path.exists(report_path):\n",
    "        with open(report_path, 'r', encoding='utf-8') as f:\n",
    "            content = f.read()\n",
    "        \n",
    "        # 分词\n",
    "        words = jieba.cut_for_search(content)\n",
    "        # 删除重复词\n",
    "        words = list(set(words))\n",
    "        # 剔除长度小于2的词和纯数字\n",
    "        words = [word for word in words if len(word.strip()) > 1 and not word.isdigit()]\n",
    "        \n",
    "        # 去除停用词并统计AI关键词的词频\n",
    "        ai_word_count = sum(word in words for word in words if word not in stopwords)\n",
    "    \n",
    "    return company_name, year, ai_word_count\n",
    "\n",
    "def count_ai_words_parallel(firm_annals_path):\n",
    "    \"\"\"并发统计所有公司年报中的AI关键词词频\"\"\"\n",
    "    file_infos = []\n",
    "    for root, dirs, files in tqdm(os.walk(firm_annals_path), desc=\"Processing files\"):\n",
    "        for file in files:\n",
    "            if file.endswith('.txt'):\n",
    "                company_name = os.path.basename(root).replace(' ', '') # 去除空格\n",
    "                year = os.path.splitext(file)[0]\n",
    "                report_path = os.path.join(root, file)\n",
    "                file_infos.append((company_name, year, report_path))\n",
    "    \n",
    "    results = []\n",
    "    with ThreadPoolExecutor() as executor:\n",
    "        futures = list(tqdm(executor.map(process_file, file_infos), total=len(file_infos), desc=\"Counting AI words\"))\n",
    "        for future in futures:\n",
    "            results.append(future)\n",
    "    \n",
    "    return results\n",
    "\n",
    "def update_word_counts(results, words_count, columns=None):\n",
    "    \"\"\"\n",
    "        更新 words_count DataFrame。\n",
    "\n",
    "        Args:\n",
    "            results (list of tuples): 处理后的数据，格式为 [(com_name, year, AI_words_count), ...]。\n",
    "            words_count (pd.DataFrame): 需要更新的现有 DataFrame。\n",
    "            columns (list of str): com_name, year 和 AI_words_count 的列名。\n",
    "        \n",
    "        Return:\n",
    "            pd.dataframe\n",
    "        \"\"\"\n",
    "    if columns is None:\n",
    "        columns = ['com_name', 'year', 'AI_words_count']\n",
    "    \n",
    "    for company_name, year, ai_word_count in results:\n",
    "        mask = (words_count[columns[0]] == company_name) & (words_count[columns[1]] == year)\n",
    "        if not words_count[mask].empty:\n",
    "            words_count.loc[mask, columns[2]] += ai_word_count\n",
    "        else:\n",
    "            new_row = pd.DataFrame([[company_name, year, ai_word_count]], columns=columns)\n",
    "            words_count = pd.concat([words_count, new_row], ignore_index=True)\n",
    "    \n",
    "    return words_count\n",
    "\n",
    "\n",
    "# 读取AI关键词典和停用词\n",
    "words = pd.read_csv('cipingdata/words_all.csv', encoding='utf-8', header=None)[0].tolist()\n",
    "stopwords = set(open('cipingdata/stopwords.txt', 'r', encoding='utf-8').read().splitlines())\n",
    "jieba.load_userdict('cipingdata/seed_dict.csv')\n",
    "\n",
    "# 初始化AI_words_count DataFrame\n",
    "words_count = pd.DataFrame(columns=['com_name', 'year', 'AI_words_count'])\n",
    "\n",
    "# 统计AI关键词词频\n",
    "firm_annals_path = 'cipingdata/testtxt'\n",
    "results = count_ai_words_parallel(firm_annals_path)\n",
    "words_count = update_word_counts(results, words_count)\n",
    "\n",
    "# 保存结果\n",
    "words_count.to_excel('words_count_result.xlsx', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
